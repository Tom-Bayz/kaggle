{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real or Not? NLP with Disaster Tweets**<br>\n",
    "https://www.kaggle.com/c/nlp-getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import numba\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== training data =====\n",
      "         id keyword location  \\\n",
      "0         1     NaN      NaN   \n",
      "1         4     NaN      NaN   \n",
      "2         5     NaN      NaN   \n",
      "3         6     NaN      NaN   \n",
      "4         7     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "7608  10869     NaN      NaN   \n",
      "7609  10870     NaN      NaN   \n",
      "7610  10871     NaN      NaN   \n",
      "7611  10872     NaN      NaN   \n",
      "7612  10873     NaN      NaN   \n",
      "\n",
      "                                                   text  target  \n",
      "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
      "1                Forest fire near La Ronge Sask. Canada       1  \n",
      "2     All residents asked to 'shelter in place' are ...       1  \n",
      "3     13,000 people receive #wildfires evacuation or...       1  \n",
      "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
      "...                                                 ...     ...  \n",
      "7608  Two giant cranes holding a bridge collapse int...       1  \n",
      "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
      "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
      "7611  Police investigating after an e-bike collided ...       1  \n",
      "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
      "\n",
      "[7613 rows x 5 columns]\n",
      "id           0.000000\n",
      "keyword      0.801261\n",
      "location    33.272035\n",
      "text         0.000000\n",
      "target       0.000000\n",
      "dtype: float64\n",
      "\n",
      "===== test data =====\n",
      "         id keyword location  \\\n",
      "0         0     NaN      NaN   \n",
      "1         2     NaN      NaN   \n",
      "2         3     NaN      NaN   \n",
      "3         9     NaN      NaN   \n",
      "4        11     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "3258  10861     NaN      NaN   \n",
      "3259  10865     NaN      NaN   \n",
      "3260  10868     NaN      NaN   \n",
      "3261  10874     NaN      NaN   \n",
      "3262  10875     NaN      NaN   \n",
      "\n",
      "                                                   text  \n",
      "0                    Just happened a terrible car crash  \n",
      "1     Heard about #earthquake is different cities, s...  \n",
      "2     there is a forest fire at spot pond, geese are...  \n",
      "3              Apocalypse lighting. #Spokane #wildfires  \n",
      "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
      "...                                                 ...  \n",
      "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
      "3259  Storm in RI worse than last hurricane. My city...  \n",
      "3260  Green Line derailment in Chicago http://t.co/U...  \n",
      "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
      "3262  #CityofCalgary has activated its Municipal Eme...  \n",
      "\n",
      "[3263 rows x 4 columns]\n",
      "id           0.000000\n",
      "keyword      0.796813\n",
      "location    33.864542\n",
      "text         0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('rawdata/train.csv')\n",
    "test = pd.read_csv('rawdata/test.csv')\n",
    "\n",
    "\n",
    "df = train\n",
    "print(\"===== training data =====\")\n",
    "print(df)\n",
    "print(df.isna().sum()/len(df)*100)\n",
    "print()\n",
    "\n",
    "df = test\n",
    "print(\"===== test data =====\")\n",
    "print(df)\n",
    "print(df.isna().sum()/len(df)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損値が多すぎて, locationが現時点で使えなさそう."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF and Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yonezu.t/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/yonezu.t/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:24: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "features = \"text\"\n",
    "target = \"target\"\n",
    "\n",
    "# prepare vectorizer\n",
    "vectorizer = TfidfVectorizer() #text => vector\n",
    "vectorizer.fit(train[features])\n",
    "\n",
    "X_train = vectorizer.transform(train[features])\n",
    "X_train = X_train.toarray()\n",
    "Y_train = train[target]\n",
    "\n",
    "# train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# predict\n",
    "data = pd.concat([train,test],axis=0)\n",
    "X = vectorizer.transform(data[features]).toarray()\n",
    "pred = model.predict(X)\n",
    "\n",
    "# concat to submit file\n",
    "data[\"target\"] = pred\n",
    "\n",
    "# load submission file\n",
    "submission = pd.read_csv(os.path.join(\"rawdata\",\"sample_submission.csv\"))\n",
    "\n",
    "# my submission file\n",
    "data = data.set_index(data[\"id\"])\n",
    "data = data.sort_index()\n",
    "\n",
    "mysub = data.iloc[submission[\"id\"]]\n",
    "mysub[[\"id\",\"target\"]].to_csv(os.path.join(\"rawdata\",\"my_submission.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF and XGboost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "features = \"text\"\n",
    "target = \"target\"\n",
    "\n",
    "# prepare vectorizer\n",
    "vectorizer = TfidfVectorizer() #text => vector\n",
    "vectorizer.fit(train[features])\n",
    "\n",
    "X_train = vectorizer.transform(train[features])\n",
    "X_train = X_train.toarray()\n",
    "Y_train = train[target]\n",
    "\n",
    "# train model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# predict\n",
    "data = pd.concat([train,test],axis=0)\n",
    "X = vectorizer.transform(data[features]).toarray()\n",
    "pred = model.predict(X)\n",
    "\n",
    "# concat to submit file\n",
    "data[\"target\"] = pred\n",
    "\n",
    "# load submission file\n",
    "submission = pd.read_csv(os.path.join(\"rawdata\",\"sample_submission.csv\"))\n",
    "\n",
    "# my submission file\n",
    "data = data.set_index(data[\"id\"])\n",
    "data = data.sort_index()\n",
    "\n",
    "mysub = data.iloc[submission[\"id\"]]\n",
    "mysub[[\"id\",\"target\"]].to_csv(os.path.join(\"rawdata\",\"my_submission.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Clean Data**  - So we have a baseline score of 79% to work with , let's get to clean data and see if we can improve the score\n",
    "\n",
    "As first step in cleaning - let us replace some commonly occuring shorthands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"i'll\", \"i will\", text)\n",
    "    text = re.sub(r\"she'll\", \"she will\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"here's\", \"here is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "df_train['clean_text'] = df_train['text'].apply(clean_text)\n",
    "df_test['clean_text'] = df_test['text'].apply(clean_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
