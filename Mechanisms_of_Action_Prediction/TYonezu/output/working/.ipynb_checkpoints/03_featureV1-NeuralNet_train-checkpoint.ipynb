{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Neural Net training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 100000\n",
    "\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df, cols):\n",
    "    for col in cols:\n",
    "        le = LabelEncoder()\n",
    "        tmp = df[col].fillna(\"NaN\")\n",
    "        df[col] = pd.Series(le.fit_transform(tmp), index=tmp.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_columns(necessary_cols,cols):\n",
    "    \n",
    "    cols = set(cols) # make set\n",
    "    \n",
    "    lack_cols = [c for c in necessary_cols if c not in cols]\n",
    "    \n",
    "    print(\"-- column check completed --\")\n",
    "    if len(lack_cols) == 0:\n",
    "        print(\"  columns are satisfied\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"  !!columns are lacked!!\")\n",
    "        print(\"   lacked columns:\",lack_cols)\n",
    "        return False\n",
    "\n",
    "\n",
    "class FeaturesMaker_v1(object):\n",
    "\n",
    "    def __init__(self,target_cols):\n",
    "        self.name = \"featuresV1\"\n",
    "        self.feature_exp = \"simple features which \"\n",
    "\n",
    "        self.target_cols = target_cols\n",
    "        self.necessary_col =  [\"sig_id\",'cp_type',\"cp_time\",\"cp_dose\",\"data_part\"] + list(target_cols)\n",
    "\n",
    "    def make_feature(self,df):\n",
    "\n",
    "        # check existstance of necessary columns\n",
    "        if check_columns(self.necessary_col,df.columns):\n",
    "\n",
    "            # label encoding\n",
    "            cols = ['cp_type',\"cp_time\",\"cp_dose\"]\n",
    "            df = label_encode(df, cols=cols)\n",
    "\n",
    "\n",
    "            # split train and test\n",
    "            df = df.set_index([\"sig_id\"],drop=True)\n",
    "\n",
    "            features = [c for c in df.columns if \"g-\" in c]\n",
    "            features = features + [c for c in df.columns if \"c-\" in c]\n",
    "            features = features + ['cp_type',\"cp_time\",\"cp_dose\"]\n",
    "\n",
    "            print(\"-- \",self.name,\" --\")\n",
    "            print(\"dim:\",len(features))\n",
    "            print(\"N:\",len(df))\n",
    "            print(\"-----------------\")\n",
    "\n",
    "            return {sub[0]:(sub[1][features],sub[1][self.target_cols]) for sub in df.groupby(by=\"data_part\")}\n",
    "\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "Dropout_Model = 0.25\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(Dropout_Model)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(Dropout_Model)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "            \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoADataset_train:\n",
    "    def __init__(self,features ,targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        dct = {\"x\":torch.tensor(self.features[idx,:], dtype=torch.float),\n",
    "               \"y\":torch.tensor(self.features[idx,:], dtype=torch.float)}\n",
    "        \n",
    "        return dct\n",
    "        \n",
    "class MoADataset_test:\n",
    "    def __init__(self,features ,targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        dct = {\"x\":torch.tensor(self.features[idx,:], dtype=torch.float)}\n",
    "        \n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "def train_model(model, optimizer, scheduler, loss_func, dataloader, device):\n",
    "    \n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data[\"x\"].to(device), data[\"y\"].to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_func(outputs,targets)\n",
    "        loss.backward()\n",
    "        optimier.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "        final_loss += loss.item()\n",
    "    \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "def valid_model(model, optimizer, scheduler, loss_func, dataloader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_pred = []    \n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = pd.read_csv(os.path.join(\"..\",\"..\",\"input\",\"lish-moa\",\"train_targets_scored.csv\"))\n",
    "train_features = pd.read_csv(os.path.join(\"..\",\"..\",\"input\",\"lish-moa\",\"train_features.csv\"))\n",
    "test_features =  pd.read_csv(os.path.join(\"..\",\"..\",\"input\",\"lish-moa\",\"test_features.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = train_targets.columns[1:]\n",
    "feature_cols = train_features.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- column check completed --\n",
      "  columns are satisfied\n",
      "--  featuresV1  --\n",
      "dim: 875\n",
      "N: 27796\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# traindata and validatoin data\n",
    "df_train = copy.copy(train_features)\n",
    "df_train = pd.merge(df_train,train_targets[[\"sig_id\"]+list(target_cols)],on=\"sig_id\",how=\"right\")\n",
    "train,valid = train_test_split(df_train)\n",
    "df_train.loc[train.index,\"data_part\"] = \"train\"\n",
    "df_train.loc[valid.index,\"data_part\"] = \"valid\"\n",
    "\n",
    "# test data\n",
    "df_test = copy.copy(test_features)\n",
    "for col in target_cols:\n",
    "    df_test[col] = np.nan\n",
    "df_test[\"data_part\"] = \"test\"\n",
    "\n",
    "# features processing\n",
    "df = pd.concat([df_train,df_test])\n",
    "\n",
    "feature_maker = FeaturesMaker_v1(target_cols=target_cols)\n",
    "df = feature_maker.make_feature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MoADataset_train(df[\"train\"][0].values, df[\"train\"][1].values)\n",
    "valid_dataset = MoADataset_train(df[\"valid\"][0].values, df[\"valid\"][1].values)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "hidden_size=1500\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (batch_norm1): BatchNorm1d(875, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dense1): Linear(in_features=875, out_features=1500, bias=True)\n",
       "  (batch_norm2): BatchNorm1d(1500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.25, inplace=False)\n",
       "  (dense2): Linear(in_features=1500, out_features=1500, bias=True)\n",
       "  (batch_norm3): BatchNorm1d(1500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout3): Dropout(p=0.25, inplace=False)\n",
       "  (dense3): Linear(in_features=1500, out_features=206, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(\n",
    "    num_features=num_features,\n",
    "    num_targets=num_targets,\n",
    "    hidden_size=hidden_size,\n",
    ")\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-ad2b0ec3b568>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mearly_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0moof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                          max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "\n",
    "early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "early_step = 0\n",
    "\n",
    "oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
    "best_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "        \n",
    "    train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "    print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "    valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "    print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "\n",
    "        best_loss = valid_loss\n",
    "        oof[val_idx] = valid_preds\n",
    "        torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "\n",
    "    elif(EARLY_STOP == True):\n",
    "\n",
    "        early_step += 1\n",
    "        if (early_step >= early_stopping_steps):\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
