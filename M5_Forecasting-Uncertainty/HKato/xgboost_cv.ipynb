{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly_express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_dir = 'rawdata'\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(os.path.join(rawdata_dir, 'sales_train_evaluation.csv'))\n",
    "sales.name = 'sales'\n",
    "calendar = pd.read_csv(os.path.join(rawdata_dir, 'calendar.csv'))\n",
    "calendar.name = 'calendar'\n",
    "prices = pd.read_csv(os.path.join(rawdata_dir, 'sell_prices.csv'))\n",
    "prices.name = 'prices'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the validation data is now available for the days 1914-1941, Adding zero sales for days: d_1942 - d_1969(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add zero sales for the remaining days 1942-1969\n",
    "for d in range(1942,1970):\n",
    "    col = 'd_' + str(d)\n",
    "    sales[col] = 0\n",
    "    sales[col] = sales[col].astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downcast in order to save memory\n",
    "def downcast(df):\n",
    "    cols = df.dtypes.index.tolist()\n",
    "    types = df.dtypes.values.tolist()\n",
    "    for i,t in enumerate(types):\n",
    "        if 'int' in str(t):\n",
    "            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int8)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int16)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int64)\n",
    "        elif 'float' in str(t):\n",
    "            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float16)\n",
    "            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float64)\n",
    "        elif t == np.object:\n",
    "            if cols[i] == 'date':\n",
    "                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype('category')\n",
    "    return df  \n",
    "\n",
    "sales = downcast(sales)\n",
    "prices = downcast(prices)\n",
    "calendar = downcast(calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine data\n",
    "Combine price data from prices dataframe and days data from calendar dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, calendar, on='d', how='left')\n",
    "df = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the categories along with their codes\n",
    "d_id = dict(zip(df.id.cat.codes, df.id))\n",
    "d_item_id = dict(zip(df.item_id.cat.codes, df.item_id))\n",
    "d_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))\n",
    "d_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))\n",
    "d_store_id = dict(zip(df.store_id.cat.codes, df.store_id))\n",
    "d_state_id = dict(zip(df.state_id.cat.codes, df.state_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\n",
    "cols = df.dtypes.index.tolist()\n",
    "types = df.dtypes.values.tolist()\n",
    "for i,type in enumerate(types):\n",
    "    if type.name == 'category':\n",
    "        df[cols[i]] = df[cols[i]].cat.codes\n",
    "        \n",
    "df.drop('date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduce lags\n",
    "def make_lags(df, lags):\n",
    "    for lag in lags:\n",
    "        df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1,2,3,6,12,24,36]\n",
    "df = make_lags(df, lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_feature(df, d_train_end = df['d'].max()):\n",
    "    # copy sold\n",
    "    sold_tmp = copy.copy(df['sold'])\n",
    "    \n",
    "    # embed NaN\n",
    "    df['sold'][df['d'] < d_train_end] = np.nan\n",
    "    \n",
    "    # make feature using sold\n",
    "    df['item_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)\n",
    "    df['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)\n",
    "    df['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)\n",
    "    df['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\n",
    "    df['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\n",
    "    df['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\n",
    "    df['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "    df['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "    df['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "    df['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)\n",
    "    df['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)\n",
    "    df['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\n",
    "    \n",
    "    df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)\n",
    "    df['expanding_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)\n",
    "    \n",
    "    df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sold'].transform('mean').astype(np.float16)\n",
    "    df['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)\n",
    "    df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)\n",
    "    df.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)\n",
    "    \n",
    "    # restore sold\n",
    "    df['sold'] = sold_tmp\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data (Use this after CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = make_new_feature(df)\n",
    "df_all = df_all[df_all['d'] >= 36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58967660 entries, 1067150 to 60034809\n",
      "Data columns (total 43 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   id                        int16  \n",
      " 1   item_id                   int16  \n",
      " 2   dept_id                   int8   \n",
      " 3   cat_id                    int8   \n",
      " 4   store_id                  int8   \n",
      " 5   state_id                  int8   \n",
      " 6   d                         int16  \n",
      " 7   sold                      float64\n",
      " 8   wm_yr_wk                  int16  \n",
      " 9   weekday                   int8   \n",
      " 10  wday                      int8   \n",
      " 11  month                     int8   \n",
      " 12  year                      int16  \n",
      " 13  event_name_1              int8   \n",
      " 14  event_type_1              int8   \n",
      " 15  event_name_2              int8   \n",
      " 16  event_type_2              int8   \n",
      " 17  snap_CA                   int8   \n",
      " 18  snap_TX                   int8   \n",
      " 19  snap_WI                   int8   \n",
      " 20  sell_price                float16\n",
      " 21  sold_lag_1                float16\n",
      " 22  sold_lag_2                float16\n",
      " 23  sold_lag_3                float16\n",
      " 24  sold_lag_6                float16\n",
      " 25  sold_lag_12               float16\n",
      " 26  sold_lag_24               float16\n",
      " 27  sold_lag_36               float16\n",
      " 28  item_sold_avg             float16\n",
      " 29  state_sold_avg            float16\n",
      " 30  store_sold_avg            float16\n",
      " 31  cat_sold_avg              float16\n",
      " 32  dept_sold_avg             float16\n",
      " 33  cat_dept_sold_avg         float16\n",
      " 34  store_item_sold_avg       float16\n",
      " 35  cat_item_sold_avg         float16\n",
      " 36  dept_item_sold_avg        float16\n",
      " 37  state_store_sold_avg      float16\n",
      " 38  state_store_cat_sold_avg  float16\n",
      " 39  store_cat_dept_sold_avg   float16\n",
      " 40  rolling_sold_mean         float16\n",
      " 41  expanding_sold_mean       float16\n",
      " 42  selling_trend             float16\n",
      "dtypes: float16(23), float64(1), int16(5), int8(14)\n",
      "memory usage: 4.7 GB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_pickle(os.path.join(data_dir, 'data_all.pkl'))\n",
    "del df_all\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LGBMRegressor(\n",
    "#     n_estimators=1000,\n",
    "#     learning_rate=0.3,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     max_depth=8,\n",
    "#     num_leaves=50,\n",
    "#     min_child_weight=300\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove evaluation from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLD_DAYS = 28\n",
    "VALID_END_DAY = 1941"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['d'] <= VALID_END_DAY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = sales.store_id.cat.codes.unique().tolist()\n",
    "result_mat = np.zeros([1, len(stores)])   # (#All-possible-params-combination) x (#store)\n",
    "fold_iter = 0\n",
    "\n",
    "### LOOP: CV fold ###\n",
    "while true\n",
    "    d_valid_end = VALID_END_DAY - NUM_FOLD_DAYS * fold_iter\n",
    "    d_train_end = VALID_END_DAY - NUM_FOLD_DAYS * (fold_iter + 1)\n",
    "    \n",
    "    df_fold = df[df['d'] <= d_valid_end]\n",
    "    df_fold = make_new_feature(df_fold, d_train_end)   # train の情報だけを使って特徴量を作成\n",
    "    \n",
    "    fold_iter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_each_store(data, params, d_train_end, d_valid_end, result_mat):\n",
    "    stores = sales.store_id.cat.codes.unique().tolist()\n",
    "    for store in stores:\n",
    "        df = data[data['store_id']==store]\n",
    "\n",
    "        #Split the data\n",
    "        X_train, y_train = df[df['d'] <= d_train_end].drop('sold',axis=1), df[df['d'] <= d_train_end]['sold']\n",
    "        X_valid, y_valid = df[(df['d'] > d_train_end) & (df['d'] <= d_valid_end)].drop('sold',axis=1), df[(df['d'] > d_train_end) & (df['d'] <= d_valid_end)]['sold']\n",
    "\n",
    "        # Train\n",
    "        model = LGBMRegressor(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            max_depth=8,\n",
    "            num_leaves=50,\n",
    "            min_child_weight=300\n",
    "        )\n",
    "        \n",
    "        print('*****Prediction for Store: {}*****'.format(d_store_id[store]))\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)],\n",
    "                 eval_metric='rmse', verbose=20, early_stopping_rounds=20)\n",
    "        valid_preds[X_valid.index] = model.predict(X_valid)\n",
    "        eval_preds[X_test.index] = model.predict(X_test)\n",
    "        filename = 'model'+str(d_store_id[store])+'.pkl'\n",
    "        # save model\n",
    "        joblib.dump(model, filename)\n",
    "        del model, X_train, y_train, X_valid, y_valid\n",
    "        gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'max_depth': 8,\n",
    "    'num_leaves': 50,\n",
    "    'min_child_weight': 300\n",
    "}\n",
    "\n",
    "param_space = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'max_depth': 8,\n",
    "    'num_leaves': 50,\n",
    "    'min_child_weight': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['max_depth']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
