{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm\n",
    "pd.set_option('max_columns', 50)\n",
    "plt.style.use('bmh')\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "INPUT_DIR = '/kaggle/input/m5-forecasting-uncertainty/'\n",
    "cal = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\n",
    "st_eval = pd.read_csv(f'{INPUT_DIR}/sales_train_evaluation.csv')\n",
    "st_valid = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')\n",
    "ss = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "sellp = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants = ['0.005', '0.025', '0.165', '0.250', '0.500', '0.750', '0.835', '0.975', '0.995']\n",
    "days = range(1, 1942)\n",
    "val_eval = ['validation', 'evaluation']\n",
    "time_series_columns = [f'd_{i}' for i in days]\n",
    "\n",
    "def CreateSales( train_sales,name_list, group):\n",
    "    '''This function returns a dataframe (sales) on the aggregation level given by name list and group'''\n",
    "    rows_ve = [(name + \"_X_\" + str(q) + \"_\" + ve, str(q)) for name in name_list for q in quants for ve in val_eval]\n",
    "    sales = train_sales.groupby(group)[time_series_columns].sum() #would not be necessary for lowest level\n",
    "    return sales\n",
    "\n",
    "def createTrainSet(sales_train_s,train_sales, name, group_level, X = False):\n",
    "    sales_total = CreateSales(train_sales,name, group_level)\n",
    "    if(X == True):\n",
    "        sales_total = sales_total.rename(index = lambda s:  s + '_X')\n",
    "    sales_train_s = sales_train_s.append(sales_total)\n",
    "    return(sales_train_s)\n",
    "\n",
    "def get_agg_df(train_sales):\n",
    "    total = ['Total']\n",
    "    train_sales['Total'] = 'Total'\n",
    "    train_sales['state_cat'] = train_sales.state_id + \"_\" + train_sales.cat_id\n",
    "    train_sales['state_dept'] = train_sales.state_id + \"_\" + train_sales.dept_id\n",
    "    train_sales['store_cat'] = train_sales.store_id + \"_\" + train_sales.cat_id\n",
    "    train_sales['store_dept'] = train_sales.store_id + \"_\" + train_sales.dept_id\n",
    "    train_sales['state_item'] = train_sales.state_id + \"_\" + train_sales.item_id\n",
    "    train_sales['item_store'] = train_sales.item_id + \"_\" + train_sales.store_id\n",
    "    total = ['Total']\n",
    "    states = ['CA', 'TX', 'WI']\n",
    "    num_stores = [('CA',4), ('TX',3), ('WI',3)]\n",
    "    stores = [x[0] + \"_\" + str(y + 1) for x in num_stores for y in range(x[1])]\n",
    "    cats = ['FOODS', 'HOBBIES', 'HOUSEHOLD']\n",
    "    num_depts = [('FOODS',3), ('HOBBIES',2), ('HOUSEHOLD',2)]\n",
    "    depts = [x[0] + \"_\" + str(y + 1) for x in num_depts for y in range(x[1])]\n",
    "    state_cats = [state + \"_\" + cat for state in states for cat in cats]\n",
    "    state_depts = [state + \"_\" + dept for state in states for dept in depts]\n",
    "    store_cats = [store + \"_\" + cat for store in stores for cat in cats]\n",
    "    store_depts = [store + \"_\" + dept for store in stores for dept in depts]\n",
    "    prods = list(train_sales.item_id.unique())\n",
    "    prod_state = [prod + \"_\" + state for prod in prods for state in states]\n",
    "    prod_store = [prod + \"_\" + store for prod in prods for store in stores]\n",
    "    cols = [i for i in train_sales.columns if i.startswith('F')]\n",
    "    sales_train_s = train_sales[cols]\n",
    "    sales_train_s = pd.DataFrame()\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, total, 'Total', X=True) #1\n",
    "    sales_train_s = createTrainSet(sales_train_s, train_sales,states, 'state_id', X=True) #2\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, stores, 'store_id', X=True) #3\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, cats, 'cat_id', X=True) #4\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, depts, 'dept_id', X=True) #5\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, state_cats, 'state_cat') #6\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, state_depts, 'state_dept') #7\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, store_cats, 'store_cat') #8\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, store_depts, 'store_dept') #9\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, prods, 'item_id', X=True) #10\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, prod_state, 'state_item') #11\n",
    "    sales_train_s = createTrainSet(sales_train_s,train_sales, prod_store, 'item_store')\n",
    "    sales_train_s['id'] = sales_train_s.index\n",
    "    return(sales_train_s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pattern:\n",
    "    Total, \n",
    "    state_id, store_id, cat_id, dept_id, item_id,\n",
    "    state_cat, store_cat, store_dept, state_item, item_store\n",
    "'''\n",
    "def get_sub_df(agg_df, st_valid, pattern):\n",
    "    d_list = list(agg_df.columns)[:-1]\n",
    "    ind_list = st_valid[pattern].unique()\n",
    "    if pattern in ['Total', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id']:\n",
    "        ind_list = [ ind + \"_X\" for ind in ind_list ]\n",
    "    return agg_df.loc[ind_list][d_list].copy()\n",
    "\n",
    "def separate_weekend(df, ind_l):\n",
    "    wend = [1, 2]\n",
    "    wday = [3,4,5,6,7]\n",
    "    wend_df = df.query('wday in @wend')\n",
    "    wday_df = df.query('wday in @wday')\n",
    "    return wend_df[ind_l], wday_df[ind_l]\n",
    "\n",
    "from scipy import stats\n",
    "def check_poisson(df):\n",
    "    unique_series = df.value_counts() # get unique items and its frequencies\n",
    "    unique_items = unique_series.index.tolist()\n",
    "\n",
    "    all_freq = len(df)\n",
    "    chi2_dof = len(unique_items) - 2 # chi2 degree of freedom\n",
    "\n",
    "    mu_hat = df.mean()\n",
    "    chi2_obs = 0\n",
    "    for x in unique_items:\n",
    "        exp_freq = stats.poisson.pmf(x, mu_hat) * all_freq\n",
    "        obs_freq = unique_series[x]\n",
    "        chi2_obs += (obs_freq - exp_freq)**2 / exp_freq\n",
    "    pval = 1.0 - stats.chi2.cdf(chi2_obs, df=chi2_dof)\n",
    "    return pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = get_agg_df(st_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = get_sub_df(agg_df, st_eval, 'item_id')\n",
    "ind_l = sub_df.index\n",
    "sub_df = sub_df.T.copy().reset_index()\n",
    "sub_df.rename(columns={'index': 'd'}, inplace=True)\n",
    "merged_df = pd.merge(sub_df, cal, on='d', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 365\n",
    "tail = merged_df.index[-1] + 1\n",
    "start = tail - window_size\n",
    "window_index = np.arange(start, tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = merged_df.reset_index().query('index in @window_index')\n",
    "wend_df_all, wday_df_all = separate_weekend(split_df, ind_l)\n",
    "alpha = 0.01\n",
    "col_list = [\"pval\", \"is_poisson\", \"mean\", \"median\", \"var\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wend_result_df = pd.DataFrame(index=ind_l, columns=col_list)\n",
    "# wend\n",
    "for data_type in tqdm(ind_l):\n",
    "    wend_df = wend_df_all[data_type]\n",
    "    pval = check_poisson(wend_df)\n",
    "    wend_result_df.loc[data_type][col_list] = [pval, pval>=alpha, wend_df.mean(), wend_df.median(), wend_df.var()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wday_result_df = pd.DataFrame(index=ind_l, columns=col_list)\n",
    "# wday\n",
    "for data_type in tqdm(ind_l):\n",
    "    wday_df = wday_df_all[data_type]\n",
    "    pval = check_poisson(wday_df)\n",
    "    wday_result_df.loc[data_type][col_list] = [pval, pval>=alpha, wday_df.mean(), wday_df.median(), wday_df.var()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wend_df_all, wday_df_all = separate_weekend(split_df, ind_l)\n",
    "#check_item = poisson_item_id[4]\n",
    "check_item = 'HOBBIES_1_002_X'\n",
    "\n",
    "print(pval_df.loc[check_item])\n",
    "\n",
    "plt.title('item {} wend'.format(check_item))\n",
    "wend_df_all[check_item].hist(bins=100, density=True)\n",
    "plt.show()\n",
    "plt.title('item {} wday'.format(check_item))\n",
    "wday_df_all[check_item].hist(bins=100, density=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}